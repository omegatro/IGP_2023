{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 19:00:26.001365: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 19:00:26.069210: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 19:00:26.070614: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-27 19:00:27.244229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "tf.get_logger().setLevel('INFO')\n",
    "from typing import Dict, Text\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article data\n",
    "art_df = pd.read_csv('../../data/processed/articles_filled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customer data\n",
    "cus_df = pd.read_csv('../../data/processed/customers_filled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full interaction dataset\n",
    "inter = pd.read_csv('../../data/transactions_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordering by date\n",
    "inter = inter.sort_values(by='t_dat').set_index('t_dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting index to datetime to filter using date ranges\n",
    "inter.index = pd.to_datetime(inter.index, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating dates to narrow-down the dataset\n",
    "latest_date = inter.index.max()\n",
    "start_of_week = latest_date - pd.Timedelta(days=7)\n",
    "prior_6_weeks = latest_date - pd.Timedelta(days=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping test data as last week and train data as last 12 month (excluding last week)\n",
    "test_inter = inter.loc[start_of_week:]\n",
    "train_inter = inter.loc[prior_6_weeks:start_of_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free RAM\n",
    "del inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping by customer-article combinations to keep only unique interactions\n",
    "train_inter = train_inter.groupby(['customer_id', 'article_id']).count().reset_index()[['customer_id', 'article_id']]\n",
    "test_inter = test_inter.groupby(['customer_id', 'article_id']).count().reset_index()[['customer_id', 'article_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding customer and product attributes\n",
    "train_df = train_inter.merge(art_df, left_on='article_id', right_on='article_id', how='left')\n",
    "test_df = test_inter.merge(art_df, left_on='article_id', right_on='article_id', how='left')\n",
    "train_df = train_df.merge(cus_df, left_on='customer_id', right_on='customer_id', how='left')\n",
    "test_df = test_df.merge(cus_df, left_on='customer_id', right_on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free RAM\n",
    "del train_inter\n",
    "del test_inter\n",
    "del art_df\n",
    "del cus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #In this iteration, only ids are used as attributes for simplicity\n",
    "# counts = pd.read_csv('../../data/processed/0_05_4312_cus_art_grp_count.csv')[['customer_id', 'article_id']].sample(250_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Adding product features\n",
    "# counts = counts.merge(art_df, left_on='article_id', right_on='article_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Adding customer features\n",
    "# counts = counts.merge(cus_df, left_on='customer_id', right_on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = train_test_split(counts, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(df):\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define customer and product features\n",
    "customer_features = ['customer_id', 'postal_code', 'club_member_status', 'fashion_news_frequency']\n",
    "product_features = ['article_id', 'prod_name', 'product_group_name', 'product_type_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[product_features+customer_features]\n",
    "test_df = test_df[product_features+customer_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = normalize_features(train_df)\n",
    "test_df = normalize_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(train_df, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test_df, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "def create_embedding_for_feature(feature_name, vocabulary, embedding_dimension=32):\n",
    "    vocabulary = [str(item) for item in vocabulary]\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.StringLookup(vocabulary=vocabulary, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(vocabulary) + 1, embedding_dimension)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {feature: create_embedding_for_feature(feature, np.unique(train_df[feature].astype('object')), embedding_dimension=embedding_dims) \n",
    "              for feature in customer_features + product_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_batches_to_check = 5  # You can adjust this number\n",
    "\n",
    "# for i, batch in enumerate(train_ds.take(num_batches_to_check)):\n",
    "#     embeddings['article_id'](batch['article_id'])\n",
    "#     print(f\"Batch {i+1}:\")\n",
    "#     for feature, value in batch.items():\n",
    "#         print(f\"  {feature}: {value.numpy()}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, feature_names):\n",
    "        super().__init__()\n",
    "        self.feature_models = [embeddings[feature] for feature in feature_names]\n",
    "        self.feature_names = feature_names\n",
    "        self.dense_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            # Add more layers if needed\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        concatenated_features = []\n",
    "        for feature in self.feature_names:\n",
    "            feature_input = inputs[feature]\n",
    "#             print('users', feature)\n",
    "            feature_model = self.feature_models[self.feature_names.index(feature)]\n",
    "            feature_embedding = feature_model(feature_input)\n",
    "            concatenated_features.append(feature_embedding)\n",
    "        concatenated = tf.concat(concatenated_features, axis=1)\n",
    "        return self.dense_layers(concatenated)\n",
    "\n",
    "class ProductModel(tf.keras.Model):\n",
    "    def __init__(self, feature_names):\n",
    "        super().__init__()\n",
    "        self.feature_models = [embeddings[feature] for feature in feature_names]\n",
    "        self.feature_names = feature_names\n",
    "        self.dense_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            # Add more layers if needed\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        concatenated_features = []\n",
    "        for feature in self.feature_names:\n",
    "            feature_input = inputs[feature]\n",
    "#             print('product', feature)\n",
    "#             print(inputs[feature])\n",
    "            feature_model = self.feature_models[self.feature_names.index(feature)]\n",
    "            feature_embedding = feature_model(feature_input)\n",
    "            concatenated_features.append(feature_embedding)\n",
    "        concatenated = tf.concat(concatenated_features, axis=1)\n",
    "        return self.dense_layers(concatenated)\n",
    "\n",
    "    def compute_embeddings(self, products):\n",
    "        return products.map(self.call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = UserModel(customer_features)\n",
    "product_model = ProductModel(product_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the unique product features for candidate embeddings\n",
    "unique_products = train_df[product_features].drop_duplicates().astype('string')\n",
    "product_features_ds = tf.data.Dataset.from_tensor_slices(dict(unique_products))\n",
    "product_embeddings = product_model.compute_embeddings(product_features_ds.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(tfrs.Model):\n",
    "    def __init__(self, user_model, product_model, product_embeddings):\n",
    "        super().__init__()\n",
    "        self.user_model = user_model\n",
    "        self.product_model = product_model\n",
    "        self.task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(candidates=product_embeddings))\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        # Extracting the features dictionary from the input tuple\n",
    "        user_embeddings = self.user_model(features)\n",
    "        positive_product_embeddings = self.product_model(features)\n",
    "\n",
    "        return self.task(user_embeddings, positive_product_embeddings, compute_metrics=not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.5\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoTowerModel(user_model, product_model, product_embeddings)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling out models for users and products\n",
    "user_model = model.user_model\n",
    "product_model = model.product_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating full dataset in order to generate embeddings for all products with trained product_model\n",
    "norm_counts = normalize_features(train_df).drop_duplicates()\n",
    "counts_ds = df_to_dataset(norm_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating product embeddings with trained product model to be used to generate recommendations\n",
    "new_prod_embeddings = product_model.compute_embeddings(counts_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining product embeddings \n",
    "product_embeddings_tensor = tf.concat([x for x in new_prod_embeddings], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting closest pairs based on top_k metric\n",
    "def find_recommendations(user_embeddings, product_embeddings, product_df=train_df, k=10):\n",
    "    scores = tf.matmul(user_embeddings, product_embeddings, transpose_b=True)\n",
    "    top_k_indices = tf.math.top_k(scores, k=k)[1]\n",
    "    recommended_product_ids = [product_df['article_id'].unique()[index] for index in top_k_indices]\n",
    "    return recommended_product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#printing user info alongside with recommendation to see if the users are truely similar\n",
    "for batch in test_ds:\n",
    "    print([(f, batch[f][:10])for f in customer_features]) \n",
    "    user_embeddings = user_model(batch)\n",
    "    recommended_products = find_recommendations(user_embeddings, product_embeddings_tensor, k=10)\n",
    "    for recommendation in recommended_products[:10]:\n",
    "        print(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def batch_generator(transactions_path, customer_df_path, product_df_path, batch_size=100):\n",
    "    # Load product dataframe\n",
    "    product_df = pd.read_csv(product_df_path)\n",
    "\n",
    "    # Process customers in batches\n",
    "    for customer_df in pd.read_csv(customer_df_path, chunksize=batch_size):        \n",
    "        #Normalizing features of customers\n",
    "        customer_df = normalize_features(customer_df)\n",
    "        customer_ds = df_to_dataset(customer_df, batch_size=batch_size)\n",
    "        \n",
    "        #Normalizing features of customers\n",
    "        product_df = normalize_features(product_df)\n",
    "        product_ds = df_to_dataset(product_df, batch_size=batch_size)\n",
    "\n",
    "        # Generate embeddings\n",
    "        user_embeddings = user_model.predict(customer_ds)\n",
    "        product_embeddings = product_model.compute_embeddings(product_ds)\n",
    "        product_embeddings = tf.concat([x for x in product_embeddings], axis=0)\n",
    "\n",
    "        # Find recommendations\n",
    "        recommended_product_ids = find_recommendations(user_embeddings, product_embeddings, product_df=product_df, k=12)\n",
    "        joined_product_ids = np.apply_along_axis(lambda x: ' '.join(map(str, x)), 1, recommended_product_ids)\n",
    "        \n",
    "        # Prepare the results\n",
    "        results = pd.DataFrame({\n",
    "            'customer_id': customer_df['customer_id'],\n",
    "            'prediction': joined_product_ids\n",
    "        })\n",
    "\n",
    "        # Yield the results for this batch\n",
    "        yield results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "generator = batch_generator('../../data/transactions_train.csv', '../../data/customers.csv', '../../data/articles.csv', batch_size=1_000)\n",
    "for predictions in generator:\n",
    "    # Process the predictions, such as saving to a CSV file\n",
    "    predictions.to_csv('../../data/predictions_test_1.csv', mode='a', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
