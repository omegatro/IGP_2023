{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trying to adapt solution from [TFRS](https://www.tensorflow.org/recommenders/examples/basic_retrieval) tutorial for H&M data."
      ],
      "metadata": {
        "id": "2PCr_mq73Jcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q scann"
      ],
      "metadata": {
        "id": "mD0kb-hgiL2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d348290c-076f-4147-bcef-b740c776e6f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "from typing import Dict, Text\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Ymlm3PE2iM8v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- H&M transaction data were subsampled using [custom function](https://github.com/omegatro/IGP_2023/blob/main/modules/preprocessing.py to 5% of the original records (seed = 4312).\n",
        "- The subsampled data were transformed to represent number of transactions for individual customer-product combinations (number of times each product was bought by each customer). The following code was used:\n",
        "```\n",
        "df = df.groupby(['customer_id', 'article_id']).count()\n",
        "df = df[['customer_id', 'article_id','t_dat']].reset_index()\n",
        "df.rename(columns={'t_dat':'t_count'}, inplace = True)\n",
        "df.to_csv('0_05_4312_cus_art_grp_count.csv', header=True, index=False)\n",
        "```\n",
        "- The resulting file was uploaded to google drive to work with from colab."
      ],
      "metadata": {
        "id": "_VJpIadL3aU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In this iteration, only ids are used as attributes for simplicity\n",
        "sample_size = 10000\n",
        "counts = pd.read_csv('/content/drive/MyDrive/0_05_4312_cus_art_grp_count.csv')[['customer_id', 'article_id']].sample(sample_size)\n",
        "#Generating training and test subsets\n",
        "train, test = train_test_split(counts, test_size=0.2)\n",
        "#Getting unique user ids (required to transform those into embeddings by the model)\n",
        "unq_cids = counts.customer_id.unique()"
      ],
      "metadata": {
        "id": "XJRYWX4VeEQx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting product ids (required to transform those into embeddings by the model)\n",
        "articles = pd.read_csv('/content/drive/MyDrive/articles.csv')[['article_id']]\n",
        "unq_articles = articles.article_id.unique().astype(str)\n",
        "article_ids = articles['article_id'].astype(str).map(lambda x: x.encode('utf-8'))"
      ],
      "metadata": {
        "id": "bYlpH7CYgHce"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(article_ids[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VxPoBPN2qxT",
        "outputId": "d5c7007d-6a94-41ef-858b-5cde5e3fb6db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    b'108775015'\n",
            "1    b'108775044'\n",
            "2    b'108775051'\n",
            "3    b'110065001'\n",
            "4    b'110065002'\n",
            "Name: article_id, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(unq_articles[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ovw4JPw2-d6",
        "outputId": "93b41cce-b748-493d-8c87-4693acedd37b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['108775015' '108775044' '108775051' '110065001' '110065002']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(unq_cids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ektO1J12f-I7",
        "outputId": "641152ce-1cc1-4018-ef64-b70ea637ca7c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9874"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(unq_articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAnpi6sieLoK",
        "outputId": "bbbe955b-e2db-473f-9a83-823a72914d23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105542"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting from pandas df to tensorflow datasets for compatibility"
      ],
      "metadata": {
        "id": "JRgf4c-c5cBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['customer_id'] = train.customer_id.astype(str)\n",
        "train['article_id'] = train.article_id.astype(str)\n",
        "train = train.to_dict(orient='list')\n",
        "train = tf.data.Dataset.from_tensor_slices(train)"
      ],
      "metadata": {
        "id": "0CaTGLR7_eg-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['customer_id'] = test.customer_id.astype(str)\n",
        "test['article_id'] = test.article_id.astype(str)\n",
        "test = test.to_dict(orient='list')\n",
        "test = tf.data.Dataset.from_tensor_slices(test)"
      ],
      "metadata": {
        "id": "g1efdOWsp-1s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in train.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaUwDqS9AruK",
        "outputId": "45af2062-71a5-462c-a67f-e4b767fefce2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'article_id': b'849859003',\n",
            " 'customer_id': b'79b2bc962d80f91406ff8e85f580ff7470741f5a53b3a49f7292eaab0866'\n",
            "                b'5479'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in test.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiEvoHALAszE",
        "outputId": "50b5c5e2-c1b0-4b3d-db62-18cd8e88ea62"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'article_id': b'758002004',\n",
            " 'customer_id': b'c5f1f946ae8be31aca5c3a38996ed94005f3402b094ff1dc497147b5c2fa'\n",
            "                b'021f'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adapting the model"
      ],
      "metadata": {
        "id": "ZfKyt08f5lDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 32"
      ],
      "metadata": {
        "id": "98pAJXkeiGcV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting features into embedding vectors\n"
      ],
      "metadata": {
        "id": "ehfeCFTL5vNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unq_cids, mask_token=None),\n",
        "  # We add an additional embedding to account for unknown tokens.\n",
        "  tf.keras.layers.Embedding(len(unq_cids) + 1, embedding_dimension)\n",
        "])"
      ],
      "metadata": {
        "id": "enbBaXAQie2a"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unq_articles, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unq_articles) + 1, embedding_dimension)\n",
        "])"
      ],
      "metadata": {
        "id": "HFuA2xCNikAk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating embeddings from article ids to be used by evaluation layer to calculate the similarity score\n",
        "article_embeddings = product_model(article_ids)\n",
        "candidates_dataset = tf.data.Dataset.from_tensor_slices(article_embeddings)"
      ],
      "metadata": {
        "id": "boyD6iSfkpei"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = tfrs.metrics.FactorizedTopK(\n",
        "    candidates=candidates_dataset.batch(128)  # Batch the candidates for efficiency\n",
        ")"
      ],
      "metadata": {
        "id": "uYCABy7oi18c"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = tfrs.tasks.Retrieval(\n",
        "  metrics=metrics\n",
        ")"
      ],
      "metadata": {
        "id": "0Yu1R-pHi39t"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MovielensModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, customer_model, product_model):\n",
        "    super().__init__()\n",
        "    self.product_model: tf.keras.Model = product_model\n",
        "    self.customer_model: tf.keras.Model = customer_model\n",
        "    self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    customer_embeddings = self.customer_model(features[\"customer_id\"])\n",
        "    # And pick out the movie features and pass them into the movie model,\n",
        "    # getting embeddings back.\n",
        "    positive_product_embeddings = self.product_model(features[\"article_id\"])\n",
        "\n",
        "    # The task computes the loss and the metrics.\n",
        "    return self.task(customer_embeddings, positive_product_embeddings)"
      ],
      "metadata": {
        "id": "FeuJRqnDlBDN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoBaseClassMovielensModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, customer_model, product_model):\n",
        "    super().__init__()\n",
        "    self.product_model: tf.keras.Model = product_model\n",
        "    self.customer_model: tf.keras.Model = customer_model\n",
        "    self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "  def train_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "\n",
        "    # Set up a gradient tape to record gradients.\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # Loss computation.\n",
        "      customer_embeddings = self.customer_model(features[\"customer_id\"])\n",
        "      positive_product_embeddings = self.product_model(features[\"article_id\"])\n",
        "      loss = self.task(customer_embeddings, positive_product_embeddings)\n",
        "\n",
        "      # Handle regularization losses as well.\n",
        "      regularization_loss = sum(self.losses)\n",
        "\n",
        "      total_loss = loss + regularization_loss\n",
        "\n",
        "    gradients = tape.gradient(total_loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
        "    metrics[\"loss\"] = loss\n",
        "    metrics[\"regularization_loss\"] = regularization_loss\n",
        "    metrics[\"total_loss\"] = total_loss\n",
        "\n",
        "    return metrics\n",
        "\n",
        "  def test_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "\n",
        "    # Loss computation.\n",
        "    customer_embeddings = self.customer_model(features[\"customer_id\"])\n",
        "    positive_product_embeddings = self.product_model(features[\"article_id\"])\n",
        "    loss = self.task(customer_embeddings, positive_product_embeddings)\n",
        "\n",
        "    # Handle regularization losses as well.\n",
        "    regularization_loss = sum(self.losses)\n",
        "\n",
        "    total_loss = loss + regularization_loss\n",
        "\n",
        "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
        "    metrics[\"loss\"] = loss\n",
        "    metrics[\"regularization_loss\"] = regularization_loss\n",
        "    metrics[\"total_loss\"] = total_loss\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "hAsaHOPgowLi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "alpha = 0.5\n",
        "batch_size = 128\n",
        "epochs = 1"
      ],
      "metadata": {
        "id": "C4VFEegBKatZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MovielensModel(customer_model, product_model)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=alpha))"
      ],
      "metadata": {
        "id": "jtxPK85KpuCG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cached_train = train.batch(batch_size).cache()\n",
        "cached_test = test.batch(batch_size).cache()"
      ],
      "metadata": {
        "id": "7X81FD08FHtb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(cached_train, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDTnTMw0p2ot",
        "outputId": "ce10f7ce-6357-4712-d467-202300943108"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 136s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0052 - factorized_top_k/top_5_categorical_accuracy: 0.0074 - factorized_top_k/top_10_categorical_accuracy: 0.0085 - factorized_top_k/top_50_categorical_accuracy: 0.0129 - factorized_top_k/top_100_categorical_accuracy: 0.0144 - loss: 609.9647 - regularization_loss: 0.0000e+00 - total_loss: 609.9647\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79241f12cc40>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "metadata": {
        "id": "kGtKjjiuGJFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb460e4-86f2-4feb-dc0c-5b0febc16966"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 33s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0110 - factorized_top_k/top_5_categorical_accuracy: 0.0125 - factorized_top_k/top_10_categorical_accuracy: 0.0150 - factorized_top_k/top_50_categorical_accuracy: 0.0225 - factorized_top_k/top_100_categorical_accuracy: 0.0300 - loss: 589.2320 - regularization_loss: 0.0000e+00 - total_loss: 589.2320\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'factorized_top_k/top_1_categorical_accuracy': 0.010999999940395355,\n",
              " 'factorized_top_k/top_5_categorical_accuracy': 0.012500000186264515,\n",
              " 'factorized_top_k/top_10_categorical_accuracy': 0.014999999664723873,\n",
              " 'factorized_top_k/top_50_categorical_accuracy': 0.02250000089406967,\n",
              " 'factorized_top_k/top_100_categorical_accuracy': 0.029999999329447746,\n",
              " 'loss': 350.56378173828125,\n",
              " 'regularization_loss': 0,\n",
              " 'total_loss': 350.56378173828125}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}